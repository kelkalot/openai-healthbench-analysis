# -*- coding: utf-8 -*-
"""healthbench-test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ROsxGAgsaq_2ThuMbfwhkL0IkTYH2CoB
"""

#code for https://medium.com/@michael_79773/a-closer-look-at-openais-new-healthbench-evaluation-benchmark-ed3455110a29

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from collections import Counter, defaultdict
import re
from tqdm.notebook import tqdm
import os
from pathlib import Path
from datetime import datetime
import io
from matplotlib.backends.backend_pdf import PdfPages
from matplotlib.figure import Figure
from matplotlib.gridspec import GridSpec
import itertools
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

plt.style.use('ggplot')
sns.set(style="whitegrid")

def load_jsonl_safely(file_path):
    """Load JSONL file with error handling"""
    print(f"Loading {file_path}...")
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"Error decoding JSON at line {i+1} in {file_path}: {e}")
    except Exception as e:
        print(f"Error reading file {file_path}: {e}")
    return data

def extract_safe(obj, key_path, default=None):
    """Safely extract nested values from a dictionary using a list of keys"""
    if not obj:
        return default
    if isinstance(key_path, str):
        key_path = [key_path]
    current = obj
    for key in key_path:
        if isinstance(current, dict) and key in current:
            current = current[key]
        elif isinstance(current, list) and isinstance(key, int) and -len(current) <= key < len(current):
            current = current[key]
        else:
            return default
    return current

def create_text_figure(title, text_content, fontsize=10):
    """Create a figure with text for inclusion in PDF"""
    fig = Figure(figsize=(8.5, 11))
    ax = fig.add_subplot(111)
    ax.axis('off')
    ax.text(0.5, 0.98, title, fontsize=14, ha='center', va='top', weight='bold', transform=ax.transAxes)
    ax.text(0.05, 0.9, text_content, fontsize=fontsize, va='top', wrap=True, transform=ax.transAxes, linespacing=1.5)
    fig.tight_layout(pad=1.5)
    return fig

def create_table_figure(title, data, column_labels, fontsize=10):
    """Create a figure with a table for inclusion in PDF"""
    fig = Figure(figsize=(8.5, 11))
    ax = fig.add_subplot(111)
    ax.axis('off')
    ax.text(0.5, 0.98, title, fontsize=14, ha='center', va='top', weight='bold', transform=ax.transAxes)
    if data:
        table = ax.table(
            cellText=data,
            colLabels=column_labels,
            loc='upper center',
            cellLoc='left',
            colWidths=[1.0/len(column_labels)] * len(column_labels) if column_labels else [1.0]
        )
        table.auto_set_font_size(False)
        table.set_fontsize(fontsize)
        table.scale(1, 1.5)
    else:
        ax.text(0.5, 0.5, "No data for table.", ha='center', va='center', transform=ax.transAxes)
    fig.tight_layout(pad=1.5)
    return fig

def analyze_dataset_structure(data, pdf=None):
    """Analyze the basic structure of the dataset"""
    if not data:
        print("No data to analyze for dataset structure")
        return {}
    counts = {
        'total_examples': len(data),
        'with_prompt': sum(1 for ex in data if 'prompt' in ex),
        'with_rubrics': sum(1 for ex in data if 'rubrics' in ex),
        'with_ideal_completions': sum(1 for ex in data if extract_safe(ex, 'ideal_completions_data') is not None),
        'with_tags': sum(1 for ex in data if 'example_tags' in ex and ex['example_tags']),
    }
    print("\n=== Dataset Structure ===")
    for key, value in counts.items():
        print(f"{key}: {value} ({value/counts.get('total_examples', 1)*100:.1f}%)")

    example = next((ex for ex in data if 'rubrics' in ex), None)
    example_info = {}
    if example:
        print("\nSample rubrics structure (first example with rubrics):")
        rubric_keys = {}
        for i, rubric in enumerate(example.get('rubrics', [])[:2]):
            print(f"\nRubric {i+1} keys: {list(rubric.keys())}")
            rubric_keys[f"rubric_{i+1}"] = list(rubric.keys())
        example_info['rubric_keys'] = rubric_keys

    if pdf is not None:
        text_content = "Dataset Structure Overview:\n\n"
        for key, value in counts.items():
            text_content += f"• {key}: {value} ({value/counts.get('total_examples', 1)*100:.1f}%)\n"
        text_content += "\nExample Structure (from first example with rubrics):\n"
        if example:
            text_content += f"• Top-level keys: {list(example.keys())}\n"
            text_content += f"• Number of rubric criteria: {len(example.get('rubrics', []))}\n"
            if 'rubric_keys' in example_info:
                for name, keys_list in example_info['rubric_keys'].items():
                    text_content += f"• {name} keys: {keys_list}\n"
        fig = create_text_figure("Dataset Structure Analysis", text_content)
        pdf.savefig(fig)
        plt.close(fig)
    return counts

def analyze_rubric_criteria(data, pdf=None, num_longest_to_show=5):
    """Analyze rubric criteria across examples"""
    print("\n=== Rubric Criteria Analysis ===")
    if not data:
        print("No data for rubric criteria analysis.")
        return {}

    criteria_count_per_example = []
    positive_criteria, negative_criteria, neutral_criteria, total_criteria = 0, 0, 0, 0
    criteria_lengths, point_values = [], []
    axis_counts, level_counts = Counter(), Counter()
    criteria_texts_and_lengths = []

    for ex in tqdm(data, desc="Analyzing rubric criteria"):
        rubrics = extract_safe(ex, 'rubrics', [])
        if not rubrics: continue
        criteria_count_per_example.append(len(rubrics))
        for criterion in rubrics:
            total_criteria += 1
            points = extract_safe(criterion, 'points')
            if points is not None:
                point_values.append(points)
                if points > 0: positive_criteria += 1
                elif points < 0: negative_criteria += 1
                else: neutral_criteria += 1

            text = extract_safe(criterion, 'criterion', '')
            if text:
                word_count = len(text.split())
                criteria_lengths.append(word_count)
                criteria_texts_and_lengths.append({'text': text, 'length': word_count, 'prompt_id': ex.get('prompt_id', 'N/A')})

            tags = extract_safe(criterion, 'tags', [])
            for tag in tags:
                if tag.startswith('axis:'): axis_counts[tag[5:]] += 1
                if tag.startswith('level:'): level_counts[tag[6:]] += 1

    results = {
        'criteria_count_per_example': criteria_count_per_example,
        'positive_criteria': positive_criteria, 'negative_criteria': negative_criteria,
        'neutral_criteria': neutral_criteria, 'total_criteria': total_criteria,
        'criteria_lengths': criteria_lengths, 'point_values': point_values,
        'axis_counts': axis_counts, 'level_counts': level_counts,
        'longest_criteria_samples': []
    }

    if criteria_count_per_example:
        print(f"Criteria per example: min={min(criteria_count_per_example)}, max={max(criteria_count_per_example)}, "
              f"avg={np.mean(criteria_count_per_example):.1f}, median={np.median(criteria_count_per_example)}")
    if total_criteria > 0:
        print(f"Criteria distribution: Positive={positive_criteria} ({positive_criteria/total_criteria*100:.1f}%), "
              f"Negative={negative_criteria} ({negative_criteria/total_criteria*100:.1f}%), "
              f"Neutral={neutral_criteria} ({neutral_criteria/total_criteria*100:.1f}%)")
    if criteria_lengths:
        print(f"Criteria text length (words): min={min(criteria_lengths)}, max={max(criteria_lengths)}, "
              f"avg={np.mean(criteria_lengths):.1f}, median={np.median(criteria_lengths)}")
        longest_criteria = sorted(criteria_texts_and_lengths, key=lambda x: x['length'], reverse=True)
        results['longest_criteria_samples'] = longest_criteria[:num_longest_to_show]
        print(f"\nTop {num_longest_to_show} longest criteria (by word count):")
        for item in results['longest_criteria_samples']:
            print(f"  Length: {item['length']} words (Example ID: {item['prompt_id']}) - Text: '{item['text'][:100]}...'")

    if pdf is not None:
        text_content_summary = "Rubric Criteria Analysis:\n\n"
        if criteria_count_per_example:
            text_content_summary += (f"Criteria per example:\n Min: {min(criteria_count_per_example)}, Max: {max(criteria_count_per_example)}, "
                                     f"Avg: {np.mean(criteria_count_per_example):.1f}, Median: {np.median(criteria_count_per_example)}\n\n")
        if total_criteria > 0:
            text_content_summary += (f"Criteria distribution:\n Positive: {positive_criteria} ({positive_criteria/total_criteria*100:.1f}%)\n "
                                     f"Negative: {negative_criteria} ({negative_criteria/total_criteria*100:.1f}%)\n "
                                     f"Neutral: {neutral_criteria} ({neutral_criteria/total_criteria*100:.1f}%)\n\n")
        if criteria_lengths:
            text_content_summary += (f"Criteria text length (words):\n Min: {min(criteria_lengths)}, Max: {max(criteria_lengths)}, "
                                     f"Avg: {np.mean(criteria_lengths):.1f}, Median: {np.median(criteria_lengths)}\n")
        if results['longest_criteria_samples']:
            text_content_summary += f"\n\nSample of Longest Criteria ({num_longest_to_show}):\n"
            for item in results['longest_criteria_samples']:
                text_content_summary += f"• Len {item['length']} (ID: {item['prompt_id']}): {item['text'][:70]}...\n"

        fig_summary = create_text_figure("Rubric Criteria Summary", text_content_summary)
        pdf.savefig(fig_summary)
        plt.close(fig_summary)

        if point_values:
            plt.figure(figsize=(10, 6)); sns.histplot(point_values, bins=20, kde=True)
            plt.title('Distribution of Rubric Criteria Points'); plt.xlabel('Points'); plt.ylabel('Frequency')
            plt.axvline(x=0, color='red', linestyle='--'); plt.tight_layout(); pdf.savefig(); plt.close()
        if axis_counts:
            plt.figure(figsize=(12, 7)); axis_data = dict(axis_counts.most_common(10))
            plt.bar(axis_data.keys(), axis_data.values()); plt.xticks(rotation=45, ha='right')
            plt.title('Top 10 Most Common Axes'); plt.ylabel('Count'); plt.tight_layout(); pdf.savefig(); plt.close()
        if level_counts:
            plt.figure(figsize=(12, 7)); level_data = dict(level_counts.most_common(10))
            plt.bar(level_data.keys(), level_data.values()); plt.xticks(rotation=45, ha='right')
            plt.title('Top 10 Most Common Levels'); plt.ylabel('Count'); plt.tight_layout(); pdf.savefig(); plt.close()
        if criteria_count_per_example:
            plt.figure(figsize=(10, 6)); sns.histplot(criteria_count_per_example, bins=30, kde=True)
            plt.title('Distribution of Criteria Count per Example'); plt.xlabel('Number of Criteria'); plt.ylabel('Frequency')
            plt.tight_layout(); pdf.savefig(); plt.close()
    return results

def analyze_themes_and_example_tags(data, pdf=None):
    print("\n=== Theme and Tag Analysis ===")
    if not data: print("No data for theme/tag analysis."); return {}
    themes, all_tags = [], []
    for ex in data:
        ex_tags = extract_safe(ex, 'example_tags', [])
        all_tags.extend(ex_tags)
        themes.extend([tag for tag in ex_tags if tag.startswith('theme:')])

    theme_counts = Counter(themes)
    tag_categories = defaultdict(int)
    for tag in all_tags:
        if ':' in tag and not tag.startswith('theme:'):
            category, _ = tag.split(':', 1)
            tag_categories[category] += 1

    if themes:
        print("Theme distribution:")
        for theme, count in theme_counts.most_common(): print(f"  {theme}: {count} ({count/len(data)*100:.1f}%)")
    if tag_categories:
        print("\nTag categories (excluding themes):")
        for cat, count in sorted(tag_categories.items(), key=lambda x: x[1], reverse=True): print(f"  {cat}: {count}")

    if pdf is not None:
        if themes:
            plt.figure(figsize=(12, 7)); theme_data = dict(sorted(theme_counts.items(),key=lambda x:x[1],reverse=True))
            plt.bar(theme_data.keys(), theme_data.values()); plt.xticks(rotation=45, ha='right')
            plt.title('Distribution of Themes'); plt.ylabel('Count'); plt.tight_layout(); pdf.savefig(); plt.close()

            theme_table_data = [[th, cnt, f"{cnt/len(data)*100:.1f}%"] for th, cnt in theme_counts.most_common()]
            fig_table = create_table_figure("Theme Distribution", theme_table_data, ["Theme", "Count", "Percentage"])
            pdf.savefig(fig_table); plt.close(fig_table)
        if tag_categories:
            plt.figure(figsize=(12, 7)); tag_cat_data = dict(sorted(tag_categories.items(),key=lambda x:x[1],reverse=True))
            plt.bar(tag_cat_data.keys(), tag_cat_data.values()); plt.xticks(rotation=45, ha='right')
            plt.title('Tag Categories (Excluding Themes)'); plt.ylabel('Count'); plt.tight_layout(); pdf.savefig(); plt.close()
    return {'theme_counts': theme_counts, 'tag_categories': tag_categories}

def analyze_prompt_structure(data, pdf=None):
    print("\n=== Prompt Structure Analysis ===")
    if not data: print("No data for prompt structure analysis."); return {}
    turn_counts, prompt_lengths, user_turns, assistant_turns = [], [], [], []
    for ex in data:
        prompt = extract_safe(ex, 'prompt', [])
        if not prompt: continue
        turn_counts.append(len(prompt))
        prompt_lengths.append(sum(len(extract_safe(turn, 'content', '')) for turn in prompt))
        roles = [extract_safe(turn, 'role', '') for turn in prompt]
        user_turns.append(sum(1 for r in roles if r == 'user'))
        assistant_turns.append(sum(1 for r in roles if r == 'assistant'))

    stats = {}
    if turn_counts:
        single_turn = sum(1 for c in turn_counts if c == 1)
        stats.update({
            'turn_counts_stats': {'min': min(turn_counts), 'max': max(turn_counts), 'avg': np.mean(turn_counts), 'median': np.median(turn_counts)},
            'single_turn_count': single_turn, 'multi_turn_count': len(turn_counts) - single_turn
        })
        print(f"Conversation turns: Min={stats['turn_counts_stats']['min']}, Max={stats['turn_counts_stats']['max']}, Avg={stats['turn_counts_stats']['avg']:.1f}, Median={stats['turn_counts_stats']['median']:.1f}")
        print(f"Single-turn: {single_turn} ({single_turn/len(turn_counts)*100:.1f}%), Multi-turn: {stats['multi_turn_count']} ({stats['multi_turn_count']/len(turn_counts)*100:.1f}%)")
    if prompt_lengths:
        stats['prompt_lengths_stats'] = {'min': min(prompt_lengths), 'max': max(prompt_lengths), 'avg': np.mean(prompt_lengths), 'median': np.median(prompt_lengths)}
        print(f"Prompt length (chars): Min={stats['prompt_lengths_stats']['min']}, Max={stats['prompt_lengths_stats']['max']}, Avg={stats['prompt_lengths_stats']['avg']:.1f}, Median={stats['prompt_lengths_stats']['median']:.1f}")

    if pdf is not None:
        text_content = "Prompt Structure Analysis:\n\n"
        if 'turn_counts_stats' in stats:
            tc = stats['turn_counts_stats']
            text_content += f"Conversation turns:\n Min: {tc['min']}, Max: {tc['max']}, Avg: {tc['avg']:.1f}, Median: {tc['median']:.1f}\n"
            text_content += f" Single-turn: {stats['single_turn_count']} ({stats['single_turn_count']/len(turn_counts)*100:.1f}%)\n"
            text_content += f" Multi-turn: {stats['multi_turn_count']} ({stats['multi_turn_count']/len(turn_counts)*100:.1f}%)\n\n"
        if 'prompt_lengths_stats' in stats:
            pl = stats['prompt_lengths_stats']
            text_content += f"Prompt length (characters):\n Min: {pl['min']}, Max: {pl['max']}, Avg: {pl['avg']:.1f}, Median: {pl['median']:.1f}\n"

        fig_text = create_text_figure("Prompt Structure Analysis", text_content)
        pdf.savefig(fig_text); plt.close(fig_text)

        if turn_counts:
            plt.figure(figsize=(10, 6)); sns.histplot(turn_counts, bins=range(1, max(turn_counts) + 2), kde=False)
            plt.title('Distribution of Conversation Turns'); plt.xlabel('Number of Turns'); plt.ylabel('Frequency')
            plt.xticks(range(1, min(max(turn_counts, default=1) + 1, 20))); plt.tight_layout(); pdf.savefig(); plt.close()
        if prompt_lengths:
            plt.figure(figsize=(10, 6)); sns.histplot(prompt_lengths, bins=50, kde=True)
            plt.title('Distribution of Prompt Lengths (characters)'); plt.xlabel('Characters'); plt.ylabel('Frequency')
            plt.tight_layout(); pdf.savefig(); plt.close()
    return stats

def calculate_text_similarity(text1, text2, vectorizer):
    """Calculates cosine similarity between two texts using a pre-fitted TF-IDF vectorizer."""
    if not text1 or not text2: return 0.0
    try:
        tfidf_matrix = vectorizer.transform([text1, text2])
        return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
    except ValueError: # Handle empty vocabulary or other TF-IDF issues
        return 0.0

def analyze_criteria_similarity(data, pdf=None, similarity_threshold=0.8):
    print("\n=== Criteria Similarity Analysis (Within Examples) ===")
    if not data: print("No data for similarity analysis."); return {}

    examples_with_similar_criteria, total_similar_pairs = 0, 0
    similarity_scores_all_examples = []

    all_criteria_texts = [extract_safe(crit, 'criterion', '') for ex in data if 'rubrics' in ex for crit in ex['rubrics'] if extract_safe(crit, 'criterion', '')]
    if not all_criteria_texts: print("No criteria text found to build TF-IDF model."); return {}
    vectorizer = TfidfVectorizer().fit(all_criteria_texts)

    for ex_idx, ex in enumerate(tqdm(data, desc="Analyzing criteria similarity")):
        rubrics = extract_safe(ex, 'rubrics', [])
        if len(rubrics) < 2: continue

        criteria_texts = [extract_safe(crit, 'criterion', '') for crit in rubrics]
        criteria_texts = [c for c in criteria_texts if c]
        if len(criteria_texts) < 2: continue

        example_had_similar_pair = False
        for i in range(len(criteria_texts)):
            for j in range(i + 1, len(criteria_texts)):
                similarity = calculate_text_similarity(criteria_texts[i], criteria_texts[j], vectorizer)
                similarity_scores_all_examples.append(similarity)
                if similarity >= similarity_threshold:
                    total_similar_pairs += 1
                    example_had_similar_pair = True
        if example_had_similar_pair: examples_with_similar_criteria += 1

    avg_similarity = np.mean(similarity_scores_all_examples) if similarity_scores_all_examples else 0
    print(f"Examples with >=1 pair of criteria similarity > {similarity_threshold}: {examples_with_similar_criteria} / {len(data)}")
    print(f"Total highly similar criteria pairs found: {total_similar_pairs}")
    print(f"Average similarity score across all pairs: {avg_similarity:.3f}")

    results = {
        'examples_with_similar_criteria': examples_with_similar_criteria,
        'total_similar_pairs': total_similar_pairs,
        'avg_similarity_score': avg_similarity,
        'similarity_threshold': similarity_threshold,
        'all_similarity_scores': similarity_scores_all_examples
    }
    if pdf and similarity_scores_all_examples:
        plt.figure(figsize=(10, 6)); sns.histplot(similarity_scores_all_examples, bins=50, kde=True)
        plt.title(f'Distribution of Within-Example Criteria Similarity Scores'); plt.xlabel('Cosine Similarity'); plt.ylabel('Frequency')
        plt.axvline(similarity_threshold, color='r', linestyle='--', label=f'Threshold ({similarity_threshold})'); plt.legend()
        plt.tight_layout(); pdf.savefig(); plt.close()
    return results

def analyze_consensus_vs_specific(main_data, consensus_data, pdf=None):
    print("\n=== Consensus vs. Example-Specific Analysis ===")
    if not main_data or not consensus_data: print("Missing data for consensus analysis."); return {}

    consensus_ids = set(extract_safe(ex, 'prompt_id') for ex in consensus_data if extract_safe(ex, 'prompt_id'))
    consensus_examples = [ex for ex in main_data if extract_safe(ex, 'prompt_id') in consensus_ids]
    non_consensus_examples = [ex for ex in main_data if extract_safe(ex, 'prompt_id') not in consensus_ids]
    print(f"Consensus examples: {len(consensus_examples)}, Non-consensus examples: {len(non_consensus_examples)}")

    def get_criteria_stats(examples_set):
        counts = [len(extract_safe(ex, 'rubrics', [])) for ex in examples_set if extract_safe(ex, 'rubrics')]
        lengths = [len(extract_safe(crit, 'criterion', '').split()) for ex in examples_set for crit in extract_safe(ex, 'rubrics', []) if extract_safe(crit, 'criterion', '')]
        return {
            'counts': counts, 'lengths': lengths,
            'min_c': min(counts) if counts else 0, 'max_c': max(counts) if counts else 0,
            'avg_c': np.mean(counts) if counts else 0, 'median_c': np.median(counts) if counts else 0,
            'min_l': min(lengths) if lengths else 0, 'max_l': max(lengths) if lengths else 0,
            'avg_l': np.mean(lengths) if lengths else 0, 'median_l': np.median(lengths) if lengths else 0,
        }

    consensus_stats = get_criteria_stats(consensus_examples)
    non_consensus_stats = get_criteria_stats(non_consensus_examples) # For non-consensus examples *from main_data*
    consensus_criteria_from_consensus_file_lengths = [len(extract_safe(crit, 'criterion', '').split()) for ex in consensus_data for crit in extract_safe(ex, 'rubrics', []) if extract_safe(crit, 'criterion', '')]

    cluster_tag_counts = Counter()
    for ex in main_data:
        for criterion in extract_safe(ex, 'rubrics', []):
            for tag in extract_safe(criterion, 'tags', []):
                if tag.startswith('cluster:'): cluster_tag_counts[tag[8:]] +=1

    print_stats = lambda name, stats_dict: print(f"\n{name} criteria count: Min={stats_dict['min_c']}, Max={stats_dict['max_c']}, Avg={stats_dict['avg_c']:.1f}, Median={stats_dict['median_c']:.1f}\n"
                                                 f"{name} criteria length (words): Min={stats_dict['min_l']}, Max={stats_dict['max_l']}, Avg={stats_dict['avg_l']:.1f}, Median={stats_dict['median_l']:.1f}")
    print_stats("Consensus examples (from main_data)", consensus_stats)
    print_stats("Non-consensus examples (from main_data)", non_consensus_stats)
    if consensus_criteria_from_consensus_file_lengths:
        print(f"\nConsensus criteria (from consensus file) text length (words): Avg={np.mean(consensus_criteria_from_consensus_file_lengths):.1f}, Median={np.median(consensus_criteria_from_consensus_file_lengths):.1f}")

    if cluster_tag_counts: print(f"\nTop consensus criteria clusters (from main_data): {cluster_tag_counts.most_common(10)}")

    results = {
        'num_consensus_examples': len(consensus_examples), 'num_non_consensus_examples': len(non_consensus_examples),
        'consensus_stats': consensus_stats, 'non_consensus_stats': non_consensus_stats,
        'cluster_tag_counts': cluster_tag_counts,
        'consensus_criteria_canonical_lengths': consensus_criteria_from_consensus_file_lengths
    }

    if pdf:
        text_content = f"Consensus vs. Example-Specific Analysis:\n\n"
        text_content += f"• Consensus examples (identified in main_data): {len(consensus_examples)}\n"
        text_content += f"• Non-consensus examples (in main_data): {len(non_consensus_examples)}\n\n"
        pdf.savefig(create_text_figure("Consensus vs. Example-Specific Summary", text_content)); plt.close()

        if consensus_stats['counts'] and non_consensus_stats['counts']:
            plt.figure(figsize=(10,6)); plt.boxplot([consensus_stats['counts'], non_consensus_stats['counts']], labels=['Consensus Examples', 'Non-Consensus Examples'])
            plt.title('Number of Criteria per Example'); plt.ylabel('Number of Criteria'); plt.tight_layout(); pdf.savefig(); plt.close()
        if consensus_criteria_from_consensus_file_lengths:
            plt.figure(figsize=(10,6)); sns.histplot(consensus_criteria_from_consensus_file_lengths, bins=30, kde=True)
            plt.title('Consensus Criteria Text Length (words - from consensus file)'); plt.xlabel('Number of Words'); plt.ylabel('Frequency'); plt.tight_layout(); pdf.savefig(); plt.close()
        if cluster_tag_counts:
            plt.figure(figsize=(12,7)); cluster_data = dict(cluster_tag_counts.most_common(10))
            plt.bar(cluster_data.keys(), cluster_data.values()); plt.xticks(rotation=45, ha='right')
            plt.title('Top 10 Consensus Criteria Clusters (from main_data tags)'); plt.ylabel('Count'); plt.tight_layout(); pdf.savefig(); plt.close()
    return results

def analyze_hard_subset(main_data, hard_data, all_examples_rubric_stats, pdf=None):
    print("\n=== Hard Subset Analysis ===")
    if not main_data or not hard_data: print("Missing data for hard subset analysis."); return {}

    hard_ids = set(extract_safe(ex, 'prompt_id') for ex in hard_data if extract_safe(ex, 'prompt_id'))
    hard_examples = [ex for ex in main_data if extract_safe(ex, 'prompt_id') in hard_ids]
    print(f"Hard examples identified in main dataset: {len(hard_examples)}")

    hard_themes, hard_axis_counts = Counter(), Counter()
    hard_point_values, hard_criteria_lengths, hard_criteria_counts = [], [], []

    for ex in hard_examples:
        hard_criteria_counts.append(len(extract_safe(ex, 'rubrics', [])))
        for tag in extract_safe(ex, 'example_tags', []):
            if tag.startswith('theme:'): hard_themes[tag] += 1
        for crit in extract_safe(ex, 'rubrics', []):
            for tag in extract_safe(crit, 'tags', []):
                if tag.startswith('axis:'): hard_axis_counts[tag[5:]] += 1
            points = extract_safe(crit, 'points');
            if points is not None: hard_point_values.append(points)
            text = extract_safe(crit, 'criterion', '');
            if text: hard_criteria_lengths.append(len(text.split()))

    all_criteria_counts = all_examples_rubric_stats.get('criteria_count_per_example', [])
    all_point_values = all_examples_rubric_stats.get('point_values', [])

    hard_stats_summary = {
        'avg_crit_count': np.mean(hard_criteria_counts) if hard_criteria_counts else 0,
        'median_crit_count': np.median(hard_criteria_counts) if hard_criteria_counts else 0,
        'avg_points': np.mean(hard_point_values) if hard_point_values else 0,
        'median_points': np.median(hard_point_values) if hard_point_values else 0,
        'avg_crit_len': np.mean(hard_criteria_lengths) if hard_criteria_lengths else 0,
        'median_crit_len': np.median(hard_criteria_lengths) if hard_criteria_lengths else 0,
    }
    all_stats_summary = {
        'avg_crit_count': np.mean(all_criteria_counts) if all_criteria_counts else 0,
        'median_crit_count': np.median(all_criteria_counts) if all_criteria_counts else 0,
    }

    print(f"Criteria count: All (avg={all_stats_summary['avg_crit_count']:.1f}, med={all_stats_summary['median_crit_count']:.1f}) vs Hard (avg={hard_stats_summary['avg_crit_count']:.1f}, med={hard_stats_summary['median_crit_count']:.1f})")
    print(f"Hard example theme dist: {hard_themes.most_common()}")
    print(f"Hard example axis dist: {hard_axis_counts.most_common(5)}")
    print(f"Hard example points: avg={hard_stats_summary['avg_points']:.1f}, med={hard_stats_summary['median_points']:.1f}")
    print(f"Hard example criteria length (words): avg={hard_stats_summary['avg_crit_len']:.1f}, med={hard_stats_summary['median_crit_len']:.1f}")

    results = {
        'num_hard_examples': len(hard_examples), 'hard_themes': hard_themes, 'hard_axis_counts': hard_axis_counts,
        'hard_criteria_counts': hard_criteria_counts, 'hard_point_values': hard_point_values,
        'hard_criteria_lengths': hard_criteria_lengths, 'summary_stats': hard_stats_summary
    }

    if pdf:
        text_content = f"Hard Subset Analysis ({len(hard_examples)} examples):\n\n"
        text_content += f"Criteria Count:\n All: Avg={all_stats_summary['avg_crit_count']:.1f}, Median={all_stats_summary['median_crit_count']:.1f}\n Hard: Avg={hard_stats_summary['avg_crit_count']:.1f}, Median={hard_stats_summary['median_crit_count']:.1f}\n\n"
        text_content += "Hard Example Theme Distribution (Top 5):\n"
        for th, cnt in hard_themes.most_common(5): text_content += f" • {th}: {cnt} ({cnt/sum(hard_themes.values())*100:.1f}%)\n"
        text_content += "\nHard Example Axis Distribution (Top 5):\n"
        for ax, cnt in hard_axis_counts.most_common(5): text_content += f" • {ax}: {cnt} ({cnt/sum(hard_axis_counts.values())*100:.1f}%)\n"
        pdf.savefig(create_text_figure("Hard Subset Analysis Summary", text_content)); plt.close()

        if hard_themes:
            plt.figure(figsize=(12,7)); theme_data = dict(sorted(hard_themes.items(),key=lambda x:x[1],reverse=True))
            plt.bar(theme_data.keys(), theme_data.values()); plt.xticks(rotation=45, ha='right')
            plt.title('Themes in Hard Dataset'); plt.ylabel('Count'); plt.tight_layout(); pdf.savefig(); plt.close()
        if all_criteria_counts and hard_criteria_counts:
            plt.figure(figsize=(10,6)); plt.boxplot([all_criteria_counts, hard_criteria_counts], labels=['All Examples', 'Hard Examples'])
            plt.title('Number of Criteria per Example (All vs Hard)'); plt.ylabel('Number of Criteria'); plt.tight_layout(); pdf.savefig(); plt.close()
        if hard_axis_counts:
            plt.figure(figsize=(12,7)); axis_data_hard = dict(hard_axis_counts.most_common(10))
            plt.bar(axis_data_hard.keys(), axis_data_hard.values()); plt.xticks(rotation=45, ha='right')
            plt.title('Top 10 Axes in Hard Examples'); plt.ylabel('Count'); plt.tight_layout(); pdf.savefig(); plt.close()
        if all_point_values and hard_point_values:
            plt.figure(figsize=(10,6)); sns.kdeplot(all_point_values, label='All Examples', fill=True, cut=0)
            sns.kdeplot(hard_point_values, label='Hard Examples', fill=True, cut=0)
            plt.title('Distribution of Criteria Points (All vs Hard)'); plt.xlabel('Points'); plt.legend(); plt.tight_layout(); pdf.savefig(); plt.close()
    return results

def create_summary_pdf_page(analysis_results, pdf):
    """Create a summary page for the PDF report based on collected analysis results."""
    text_content = "HealthBench Comprehensive Analysis Summary\n\n"
    ds = analysis_results.get('dataset_structure', {})
    rs = analysis_results.get('rubric_stats', {})
    ps = analysis_results.get('prompt_structure', {})
    cs = analysis_results.get('consensus_vs_specific', {})
    hs = analysis_results.get('hard_analysis', {})
    sim_res = analysis_results.get('criteria_similarity', {})

    text_content += f"Main dataset: {ds.get('total_examples', 'N/A')} examples\n"
    if 'num_consensus_examples' in cs:
        text_content += f"Consensus examples (in main): {cs.get('num_consensus_examples', 'N/A')}\n"
    if 'num_hard_examples' in hs:
        text_content += f"Hard examples (in main): {hs.get('num_hard_examples', 'N/A')}\n\n"

    text_content += "Key Rubric Findings:\n"
    if rs.get('criteria_count_per_example'):
        cc = rs['criteria_count_per_example']
        text_content += f"• Criteria/example: Min={min(cc)}, Max={max(cc)}, Avg={np.mean(cc):.1f}\n"
    if rs.get('total_criteria', 0) > 0:
        tc = rs['total_criteria']
        text_content += f"• {rs.get('positive_criteria',0)} positive ({rs.get('positive_criteria',0)/tc*100:.1f}%) & {rs.get('negative_criteria',0)} negative ({rs.get('negative_criteria',0)/tc*100:.1f}%) criteria\n"
    if rs.get('criteria_lengths'):
        cl = rs['criteria_lengths']
        text_content += f"• Criteria length (words): Avg={np.mean(cl):.1f}, Median={np.median(cl):.1f}\n"

    if 'avg_similarity_score' in sim_res:
         text_content += f"\nCriteria Similarity (within examples):\n"
         text_content += f"• Avg similarity: {sim_res['avg_similarity_score']:.3f}\n"
         text_content += f"• Examples with pair > {sim_res['similarity_threshold']}: {sim_res['examples_with_similar_criteria']}\n"

    if 'turn_counts_stats' in ps:
        text_content += f"\nPrompt Structure:\n• Avg turns: {ps['turn_counts_stats']['avg']:.1f}\n"
        text_content += f"• Single-turn conversations: {ps.get('single_turn_count', 'N/A')} ({ps.get('single_turn_count',0)/(ps.get('single_turn_count',0)+ps.get('multi_turn_count',1))*100:.1f}%)\n"

    fig = create_text_figure("HealthBench Analysis Overall Summary", text_content, fontsize=11)
    pdf.savefig(fig)
    plt.close(fig)

def analyze_healthbench(main_file, consensus_file, hard_file, output_pdf_path=None):
    """Run comprehensive analysis on HealthBench datasets and save results to PDF"""
    main_data = load_jsonl_safely(main_file)
    consensus_data = load_jsonl_safely(consensus_file)
    hard_data = load_jsonl_safely(hard_file)

    if not main_data: print("Error: Main dataset could not be loaded. Aborting."); return {}
    print(f"\nLoaded main: {len(main_data)}, consensus: {len(consensus_data)}, hard: {len(hard_data)} examples.")

    all_results = {'main_data_len': len(main_data)}

    pdf = None
    if output_pdf_path:
        pdf = PdfPages(output_pdf_path)
        print(f"Will save PDF report to: {output_pdf_path}")

    try:
        all_results['dataset_structure'] = analyze_dataset_structure(main_data, pdf)
        all_results['theme_analysis'] = analyze_themes_and_example_tags(main_data, pdf)
        all_results['prompt_structure'] = analyze_prompt_structure(main_data, pdf)
        all_results['rubric_stats'] = analyze_rubric_criteria(main_data, pdf, num_longest_to_show=5)

        if main_data: # Criteria similarity requires main_data
            all_results['criteria_similarity'] = analyze_criteria_similarity(main_data, pdf, similarity_threshold=0.8)

        if consensus_data: # Consensus vs specific requires main_data and consensus_data
            all_results['consensus_vs_specific'] = analyze_consensus_vs_specific(main_data, consensus_data, pdf)

        if hard_data and all_results.get('rubric_stats'): # Hard subset needs main_data, hard_data, and all_examples_rubric_stats
            all_results['hard_analysis'] = analyze_hard_subset(main_data, hard_data, all_results['rubric_stats'], pdf)

        # Create summary page using all_results collected so far
        if pdf:
            create_summary_pdf_page(all_results, pdf) # This will be the last page added

        print("\n=== Full Analysis Complete ===")

    except Exception as e:
        print(f"An error occurred during analysis: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if pdf:
            d = pdf.infodict()
            d['Title'] = 'HealthBench Enhanced Analysis Report'
            d['Author'] = 'Automated Analysis Script'
            d['Subject'] = 'In-depth analysis of the HealthBench Dataset'
            d['CreationDate'] = datetime.now()
            d['ModDate'] = datetime.now()
            pdf.close()
            print(f"Analysis report saved to {output_pdf_path}")

    return all_results

if __name__ == '__main__':
    main_jsonl_file = "2025-05-07-06-14-12_oss_eval.jsonl"
    consensus_jsonl_file = "consensus_2025-05-09-20-00-46.jsonl"
    hard_jsonl_file = "hard_2025-05-08-21-00-10.jsonl"
    pdf_output_file = "healthbench_enhanced_analysis_report.pdf"

    # Check if files exist, if not, create dummy ones for testing
    for f_path in [main_jsonl_file, consensus_jsonl_file, hard_jsonl_file]:
        if not os.path.exists(f_path):
            print(f"Warning: File {f_path} not found. Creating a dummy empty JSONL file for testing.")
            try:
                # Create a minimal valid JSONL structure for testing basic flow
                dummy_data = [{
                    "prompt_id": "dummy_prompt_0",
                    "prompt": [{"role": "user", "content": "Hello"}],
                    "rubrics": [
                        {"criterion": "Criterion A for dummy 0", "points": 5, "tags": ["axis:accuracy", "theme:global_health"]},
                        {"criterion": "Criterion B for dummy 0", "points": -2, "tags": ["axis:completeness"]}
                    ],
                    "example_tags": ["theme:global_health"]
                }]
                if "consensus" in f_path :
                     dummy_data[0]["rubrics"][0]["tags"].append("cluster:some_cluster")
                with open(f_path, 'w') as f:
                    for item in dummy_data:
                        f.write(json.dumps(item) + '\n')

            except Exception as e:
                print(f"Could not create dummy file {f_path}: {e}")


    analysis_summary = analyze_healthbench(
        main_jsonl_file,
        consensus_jsonl_file,
        hard_jsonl_file,
        pdf_output_file
    )
    print("\nAccess detailed results in the 'analysis_summary' dictionary (if run in interactive mode).")

